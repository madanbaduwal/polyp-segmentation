{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMBYVSBYa3bTdpfnnTi94bb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# !pip install timm # install the missing module"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"uVIQGk76QPdu","executionInfo":{"status":"ok","timestamp":1727419979139,"user_tz":300,"elapsed":4660,"user":{"displayName":"Tensor Baduwal","userId":"06083011424774211651"}},"outputId":"35490732-6344-4188-8245-ae2b11f3b1e3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting timm\n","  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.1+cu121)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.7)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (10.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n","Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: timm\n","Successfully installed timm-1.0.9\n"]}]},{"cell_type":"markdown","source":["# Data load"],"metadata":{"id":"xVts7ej2PXYu"}},{"cell_type":"code","source":["import os\n","from PIL import Image\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","import numpy as np\n","import random\n","import torch\n","\n","\n","class PolypDataset(data.Dataset):\n","    \"\"\"\n","    dataloader for polyp segmentation tasks\n","    \"\"\"\n","    def __init__(self, image_root, gt_root, trainsize, augmentations):\n","        self.trainsize = trainsize\n","        self.augmentations = augmentations\n","        print(self.augmentations)\n","        self.images = [image_root + f for f in os.listdir(image_root) if f.endswith('.jpg') or f.endswith('.png')]\n","        self.gts = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.png')]\n","        self.images = sorted(self.images)\n","        self.gts = sorted(self.gts)\n","        self.filter_files()\n","        self.size = len(self.images)\n","        if self.augmentations == 'True':\n","            print('Using RandomRotation, RandomFlip')\n","            self.img_transform = transforms.Compose([\n","                transforms.RandomRotation(90, resample=False, expand=False, center=None, fill=None),\n","                transforms.RandomVerticalFlip(p=0.5),\n","                transforms.RandomHorizontalFlip(p=0.5),\n","                transforms.Resize((self.trainsize, self.trainsize)),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.485, 0.456, 0.406],\n","                                     [0.229, 0.224, 0.225])])\n","            self.gt_transform = transforms.Compose([\n","                transforms.RandomRotation(90, resample=False, expand=False, center=None, fill=None),\n","                transforms.RandomVerticalFlip(p=0.5),\n","                transforms.RandomHorizontalFlip(p=0.5),\n","                transforms.Resize((self.trainsize, self.trainsize)),\n","                transforms.ToTensor()])\n","\n","        else:\n","            print('no augmentation')\n","            self.img_transform = transforms.Compose([\n","                transforms.Resize((self.trainsize, self.trainsize)),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.485, 0.456, 0.406],\n","                                     [0.229, 0.224, 0.225])])\n","\n","            self.gt_transform = transforms.Compose([\n","                transforms.Resize((self.trainsize, self.trainsize)),\n","                transforms.ToTensor()])\n","\n","\n","    def __getitem__(self, index):\n","\n","        image = self.rgb_loader(self.images[index])\n","        gt = self.binary_loader(self.gts[index])\n","\n","        seed = np.random.randint(2147483647) # make a seed with numpy generator\n","        random.seed(seed) # apply this seed to img tranfsorms\n","        torch.manual_seed(seed) # needed for torchvision 0.7\n","        if self.img_transform is not None:\n","            image = self.img_transform(image)\n","\n","        random.seed(seed) # apply this seed to img tranfsorms\n","        torch.manual_seed(seed) # needed for torchvision 0.7\n","        if self.gt_transform is not None:\n","            gt = self.gt_transform(gt)\n","        return image, gt\n","\n","    def filter_files(self):\n","        assert len(self.images) == len(self.gts)\n","        images = []\n","        gts = []\n","        for img_path, gt_path in zip(self.images, self.gts):\n","            img = Image.open(img_path)\n","            gt = Image.open(gt_path)\n","            if img.size == gt.size:\n","                images.append(img_path)\n","                gts.append(gt_path)\n","        self.images = images\n","        self.gts = gts\n","\n","    def rgb_loader(self, path):\n","        with open(path, 'rb') as f:\n","            img = Image.open(f)\n","            return img.convert('RGB')\n","\n","    def binary_loader(self, path):\n","        with open(path, 'rb') as f:\n","            img = Image.open(f)\n","            # return img.convert('1')\n","            return img.convert('L')\n","\n","    def resize(self, img, gt):\n","        assert img.size == gt.size\n","        w, h = img.size\n","        if h < self.trainsize or w < self.trainsize:\n","            h = max(h, self.trainsize)\n","            w = max(w, self.trainsize)\n","            return img.resize((w, h), Image.BILINEAR), gt.resize((w, h), Image.NEAREST)\n","        else:\n","            return img, gt\n","\n","    def __len__(self):\n","        return self.size\n","\n","\n","def get_loader(image_root, gt_root, batchsize, trainsize, shuffle=True, num_workers=4, pin_memory=True, augmentation=False):\n","\n","    dataset = PolypDataset(image_root, gt_root, trainsize, augmentation)\n","    data_loader = data.DataLoader(dataset=dataset,\n","                                  batch_size=batchsize,\n","                                  shuffle=shuffle,\n","                                  num_workers=num_workers,\n","                                  pin_memory=pin_memory)\n","    return data_loader\n","\n","\n","class test_dataset:\n","    def __init__(self, image_root, gt_root, testsize):\n","        self.testsize = testsize\n","        self.images = [image_root + f for f in os.listdir(image_root) if f.endswith('.jpg') or f.endswith('.png')]\n","        self.gts = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.tif') or f.endswith('.png')]\n","        self.images = sorted(self.images)\n","        self.gts = sorted(self.gts)\n","        self.transform = transforms.Compose([\n","            transforms.Resize((self.testsize, self.testsize)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406],\n","                                 [0.229, 0.224, 0.225])])\n","        self.gt_transform = transforms.ToTensor()\n","        self.size = len(self.images)\n","        self.index = 0\n","\n","    def load_data(self):\n","        image = self.rgb_loader(self.images[self.index])\n","        image = self.transform(image).unsqueeze(0)\n","        gt = self.binary_loader(self.gts[self.index])\n","        name = self.images[self.index].split('/')[-1]\n","        if name.endswith('.jpg'):\n","            name = name.split('.jpg')[0] + '.png'\n","        self.index += 1\n","        return image, gt, name\n","\n","    def rgb_loader(self, path):\n","        with open(path, 'rb') as f:\n","            img = Image.open(f)\n","            return img.convert('RGB')\n","\n","    def binary_loader(self, path):\n","        with open(path, 'rb') as f:\n","            img = Image.open(f)\n","            return img.convert('L')"],"metadata":{"id":"-PrBMJGHPkzb","executionInfo":{"status":"ok","timestamp":1727420269345,"user_tz":300,"elapsed":254,"user":{"displayName":"Tensor Baduwal","userId":"06083011424774211651"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Train model"],"metadata":{"id":"adLAqcuZPSDy"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from functools import partial\n","\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","from timm.models.registry import register_model\n","from timm.models.vision_transformer import _cfg\n","from timm.models.registry import register_model\n","\n","import math\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.dwconv = DWConv(hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        x = self.fc1(x)\n","        x = self.dwconv(x, H, W)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n","        super().__init__()\n","        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n","\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n","        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        self.sr_ratio = sr_ratio\n","        if sr_ratio > 1:\n","            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n","            self.norm = nn.LayerNorm(dim)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        B, N, C = x.shape\n","        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n","\n","        if self.sr_ratio > 1:\n","            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n","            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n","            x_ = self.norm(x_)\n","            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        else:\n","            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        k, v = kv[0], kv[1]\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","\n","        return x\n","\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(\n","            dim,\n","            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n","        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n","        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n","\n","        return x\n","\n","\n","class OverlapPatchEmbed(nn.Module):\n","    \"\"\" Image to Patch Embedding\n","    \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n","        self.num_patches = self.H * self.W\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n","                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x):\n","        x = self.proj(x)\n","        _, _, H, W = x.shape\n","        x = x.flatten(2).transpose(1, 2)\n","        x = self.norm(x)\n","\n","        return x, H, W\n","\n","\n","class PyramidVisionTransformerImpr(nn.Module):\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n","                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n","                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n","                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1]):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.depths = depths\n","\n","        # patch_embed\n","        self.patch_embed1 = OverlapPatchEmbed(img_size=img_size, patch_size=7, stride=4, in_chans=in_chans,\n","                                              embed_dim=embed_dims[0])\n","        self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n","                                              embed_dim=embed_dims[1])\n","        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n","                                              embed_dim=embed_dims[2])\n","        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 16, patch_size=3, stride=2, in_chans=embed_dims[2],\n","                                              embed_dim=embed_dims[3])\n","\n","        # transformer encoder\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n","        cur = 0\n","        self.block1 = nn.ModuleList([Block(\n","            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[0])\n","            for i in range(depths[0])])\n","        self.norm1 = norm_layer(embed_dims[0])\n","\n","        cur += depths[0]\n","        self.block2 = nn.ModuleList([Block(\n","            dim=embed_dims[1], num_heads=num_heads[1], mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[1])\n","            for i in range(depths[1])])\n","        self.norm2 = norm_layer(embed_dims[1])\n","\n","        cur += depths[1]\n","        self.block3 = nn.ModuleList([Block(\n","            dim=embed_dims[2], num_heads=num_heads[2], mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[2])\n","            for i in range(depths[2])])\n","        self.norm3 = norm_layer(embed_dims[2])\n","\n","        cur += depths[2]\n","        self.block4 = nn.ModuleList([Block(\n","            dim=embed_dims[3], num_heads=num_heads[3], mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[3])\n","            for i in range(depths[3])])\n","        self.norm4 = norm_layer(embed_dims[3])\n","\n","        # classification head\n","        # self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def init_weights(self, pretrained=None):\n","        if isinstance(pretrained, str):\n","            logger = 1\n","            #load_checkpoint(self, pretrained, map_location='cpu', strict=False, logger=logger)\n","\n","    def reset_drop_path(self, drop_path_rate):\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(self.depths))]\n","        cur = 0\n","        for i in range(self.depths[0]):\n","            self.block1[i].drop_path.drop_prob = dpr[cur + i]\n","\n","        cur += self.depths[0]\n","        for i in range(self.depths[1]):\n","            self.block2[i].drop_path.drop_prob = dpr[cur + i]\n","\n","        cur += self.depths[1]\n","        for i in range(self.depths[2]):\n","            self.block3[i].drop_path.drop_prob = dpr[cur + i]\n","\n","        cur += self.depths[2]\n","        for i in range(self.depths[3]):\n","            self.block4[i].drop_path.drop_prob = dpr[cur + i]\n","\n","    def freeze_patch_emb(self):\n","        self.patch_embed1.requires_grad = False\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'}  # has pos_embed may be better\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    # def _get_pos_embed(self, pos_embed, patch_embed, H, W):\n","    #     if H * W == self.patch_embed1.num_patches:\n","    #         return pos_embed\n","    #     else:\n","    #         return F.interpolate(\n","    #             pos_embed.reshape(1, patch_embed.H, patch_embed.W, -1).permute(0, 3, 1, 2),\n","    #             size=(H, W), mode=\"bilinear\").reshape(1, -1, H * W).permute(0, 2, 1)\n","\n","    def forward_features(self, x):\n","        B = x.shape[0]\n","        outs = []\n","\n","        # stage 1\n","        x, H, W = self.patch_embed1(x)\n","        for i, blk in enumerate(self.block1):\n","            x = blk(x, H, W)\n","        x = self.norm1(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 2\n","        x, H, W = self.patch_embed2(x)\n","        for i, blk in enumerate(self.block2):\n","            x = blk(x, H, W)\n","        x = self.norm2(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 3\n","        x, H, W = self.patch_embed3(x)\n","        for i, blk in enumerate(self.block3):\n","            x = blk(x, H, W)\n","        x = self.norm3(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 4\n","        x, H, W = self.patch_embed4(x)\n","        for i, blk in enumerate(self.block4):\n","            x = blk(x, H, W)\n","        x = self.norm4(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        return outs\n","\n","        # return x.mean(dim=1)\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        # x = self.head(x)\n","\n","        return x\n","\n","\n","class DWConv(nn.Module):\n","    def __init__(self, dim=768):\n","        super(DWConv, self).__init__()\n","        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n","\n","    def forward(self, x, H, W):\n","        B, N, C = x.shape\n","        x = x.transpose(1, 2).view(B, C, H, W)\n","        x = self.dwconv(x)\n","        x = x.flatten(2).transpose(1, 2)\n","\n","        return x\n","\n","\n","def _conv_filter(state_dict, patch_size=16):\n","    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n","    out_dict = {}\n","    for k, v in state_dict.items():\n","        if 'patch_embed.proj.weight' in k:\n","            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n","        out_dict[k] = v\n","\n","    return out_dict\n","\n","\n","@register_model\n","class pvt_v2_b0(PyramidVisionTransformerImpr):\n","    def __init__(self, **kwargs):\n","        super(pvt_v2_b0, self).__init__(\n","            patch_size=4, embed_dims=[32, 64, 160, 256], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","\n","@register_model\n","class pvt_v2_b1(PyramidVisionTransformerImpr):\n","    def __init__(self, **kwargs):\n","        super(pvt_v2_b1, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","@register_model\n","class pvt_v2_b2(PyramidVisionTransformerImpr):\n","    def __init__(self, **kwargs):\n","        super(pvt_v2_b2, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","@register_model\n","class pvt_v2_b3(PyramidVisionTransformerImpr):\n","    def __init__(self, **kwargs):\n","        super(pvt_v2_b3, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","@register_model\n","class pvt_v2_b4(PyramidVisionTransformerImpr):\n","    def __init__(self, **kwargs):\n","        super(pvt_v2_b4, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","@register_model\n","class pvt_v2_b5(PyramidVisionTransformerImpr):\n","    def __init__(self, **kwargs):\n","        super(pvt_v2_b5, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 6, 40, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pHEOrzCxP2qU","executionInfo":{"status":"ok","timestamp":1727419990906,"user_tz":300,"elapsed":4383,"user":{"displayName":"Tensor Baduwal","userId":"06083011424774211651"}},"outputId":"86be9545-3647-47fc-c033-ae710c3eb318"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-0f50936f030e>:388: UserWarning: Overwriting pvt_v2_b0 in registry with __main__.pvt_v2_b0. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  class pvt_v2_b0(PyramidVisionTransformerImpr):\n","<ipython-input-3-0f50936f030e>:398: UserWarning: Overwriting pvt_v2_b1 in registry with __main__.pvt_v2_b1. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  class pvt_v2_b1(PyramidVisionTransformerImpr):\n","<ipython-input-3-0f50936f030e>:406: UserWarning: Overwriting pvt_v2_b2 in registry with __main__.pvt_v2_b2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  class pvt_v2_b2(PyramidVisionTransformerImpr):\n","<ipython-input-3-0f50936f030e>:414: UserWarning: Overwriting pvt_v2_b3 in registry with __main__.pvt_v2_b3. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  class pvt_v2_b3(PyramidVisionTransformerImpr):\n","<ipython-input-3-0f50936f030e>:422: UserWarning: Overwriting pvt_v2_b4 in registry with __main__.pvt_v2_b4. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  class pvt_v2_b4(PyramidVisionTransformerImpr):\n","<ipython-input-3-0f50936f030e>:431: UserWarning: Overwriting pvt_v2_b5 in registry with __main__.pvt_v2_b5. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  class pvt_v2_b5(PyramidVisionTransformerImpr):\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","# from lib.pvtv2 import pvt_v2_b2,pvt_v2_b0\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BasicConv2d(nn.Module):\n","    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n","        super(BasicConv2d, self).__init__()\n","\n","        self.conv = nn.Conv2d(in_planes, out_planes,\n","                              kernel_size=kernel_size, stride=stride,\n","                              padding=padding, dilation=dilation, bias=False)\n","        self.bn = nn.BatchNorm2d(out_planes)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        return x\n","\n","\n","class CFM(nn.Module):\n","    def __init__(self, channel):\n","        super(CFM, self).__init__()\n","        self.relu = nn.ReLU(True)\n","\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","        self.conv_upsample1 = BasicConv2d(channel, channel, 3, padding=1)\n","        self.conv_upsample2 = BasicConv2d(channel, channel, 3, padding=1)\n","        self.conv_upsample3 = BasicConv2d(channel, channel, 3, padding=1)\n","        self.conv_upsample4 = BasicConv2d(channel, channel, 3, padding=1)\n","        self.conv_upsample5 = BasicConv2d(2 * channel, 2 * channel, 3, padding=1)\n","\n","        self.conv_concat2 = BasicConv2d(2 * channel, 2 * channel, 3, padding=1)\n","        self.conv_concat3 = BasicConv2d(3 * channel, 3 * channel, 3, padding=1)\n","        self.conv4 = BasicConv2d(3 * channel, channel, 3, padding=1)\n","\n","    def forward(self, x1, x2, x3):\n","        x1_1 = x1\n","        x2_1 = self.conv_upsample1(self.upsample(x1)) * x2\n","        x3_1 = self.conv_upsample2(self.upsample(self.upsample(x1))) \\\n","               * self.conv_upsample3(self.upsample(x2)) * x3\n","\n","        x2_2 = torch.cat((x2_1, self.conv_upsample4(self.upsample(x1_1))), 1)\n","        x2_2 = self.conv_concat2(x2_2)\n","\n","        x3_2 = torch.cat((x3_1, self.conv_upsample5(self.upsample(x2_2))), 1)\n","        x3_2 = self.conv_concat3(x3_2)\n","\n","        x1 = self.conv4(x3_2)\n","\n","        return x1\n","\n","\n","\n","\n","class GCN(nn.Module):\n","    def __init__(self, num_state, num_node, bias=False):\n","        super(GCN, self).__init__()\n","        self.conv1 = nn.Conv1d(num_node, num_node, kernel_size=1)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv1d(num_state, num_state, kernel_size=1, bias=bias)\n","\n","    def forward(self, x):\n","        h = self.conv1(x.permute(0, 2, 1)).permute(0, 2, 1)\n","        h = h - x\n","        h = self.relu(self.conv2(h))\n","        return h\n","\n","\n","class SAM(nn.Module):\n","    def __init__(self, num_in=32, plane_mid=16, mids=4, normalize=False):\n","        super(SAM, self).__init__()\n","\n","        self.normalize = normalize\n","        self.num_s = int(plane_mid)\n","        self.num_n = (mids) * (mids)\n","        self.priors = nn.AdaptiveAvgPool2d(output_size=(mids + 2, mids + 2))\n","\n","        self.conv_state = nn.Conv2d(num_in, self.num_s, kernel_size=1)\n","        self.conv_proj = nn.Conv2d(num_in, self.num_s, kernel_size=1)\n","        self.gcn = GCN(num_state=self.num_s, num_node=self.num_n)\n","        self.conv_extend = nn.Conv2d(self.num_s, num_in, kernel_size=1, bias=False)\n","\n","    def forward(self, x, edge):\n","        edge = F.upsample(edge, (x.size()[-2], x.size()[-1]))\n","\n","        n, c, h, w = x.size()\n","        edge = torch.nn.functional.softmax(edge, dim=1)[:, 1, :, :].unsqueeze(1)\n","\n","        x_state_reshaped = self.conv_state(x).view(n, self.num_s, -1)\n","        x_proj = self.conv_proj(x)\n","        x_mask = x_proj * edge\n","\n","        x_anchor1 = self.priors(x_mask)\n","        x_anchor2 = self.priors(x_mask)[:, :, 1:-1, 1:-1].reshape(n, self.num_s, -1)\n","        x_anchor = self.priors(x_mask)[:, :, 1:-1, 1:-1].reshape(n, self.num_s, -1)\n","\n","        x_proj_reshaped = torch.matmul(x_anchor.permute(0, 2, 1), x_proj.reshape(n, self.num_s, -1))\n","        x_proj_reshaped = torch.nn.functional.softmax(x_proj_reshaped, dim=1)\n","\n","        x_rproj_reshaped = x_proj_reshaped\n","\n","        x_n_state = torch.matmul(x_state_reshaped, x_proj_reshaped.permute(0, 2, 1))\n","        if self.normalize:\n","            x_n_state = x_n_state * (1. / x_state_reshaped.size(2))\n","        x_n_rel = self.gcn(x_n_state)\n","\n","        x_state_reshaped = torch.matmul(x_n_rel, x_rproj_reshaped)\n","        x_state = x_state_reshaped.view(n, self.num_s, *x.size()[2:])\n","        out = x + (self.conv_extend(x_state))\n","\n","        return out\n","\n","\n","class ChannelAttention(nn.Module):\n","    def __init__(self, in_planes, ratio=16):\n","        super(ChannelAttention, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.max_pool = nn.AdaptiveMaxPool2d(1)\n","\n","        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n","        self.relu1 = nn.ReLU()\n","        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n","\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n","        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n","        out = avg_out + max_out\n","        return self.sigmoid(out)\n","\n","\n","class SpatialAttention(nn.Module):\n","    def __init__(self, kernel_size=7):\n","        super(SpatialAttention, self).__init__()\n","\n","        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n","        padding = 3 if kernel_size == 7 else 1\n","\n","        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = torch.mean(x, dim=1, keepdim=True)\n","        max_out, _ = torch.max(x, dim=1, keepdim=True)\n","        x = torch.cat([avg_out, max_out], dim=1)\n","        x = self.conv1(x)\n","        return self.sigmoid(x)\n","\n","\n","class PolypPVT(nn.Module):\n","    def __init__(self, channel=32):\n","        super(PolypPVT, self).__init__()\n","\n","        self.backbone = pvt_v2_b2()  # [64, 128, 320, 512]\n","        path = './pretrained_pth/pvt_v2_b2.pth'\n","        save_model = torch.load(path)\n","        model_dict = self.backbone.state_dict()\n","        state_dict = {k: v for k, v in save_model.items() if k in model_dict.keys()}\n","        model_dict.update(state_dict)\n","        self.backbone.load_state_dict(model_dict)\n","\n","        self.Translayer2_0 = BasicConv2d(64, channel, 1)\n","        self.Translayer2_1 = BasicConv2d(128, channel, 1)\n","        self.Translayer3_1 = BasicConv2d(320, channel, 1)\n","        self.Translayer4_1 = BasicConv2d(512, channel, 1)\n","\n","        self.CFM = CFM(channel)\n","        self.ca = ChannelAttention(64)\n","        self.sa = SpatialAttention()\n","        self.SAM = SAM()\n","\n","        self.down05 = nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=True)\n","        self.out_SAM = nn.Conv2d(channel, 1, 1)\n","        self.out_CFM = nn.Conv2d(channel, 1, 1)\n","\n","\n","    def forward(self, x):\n","\n","        # backbone\n","        pvt = self.backbone(x)\n","        x1 = pvt[0]\n","        x2 = pvt[1]\n","        x3 = pvt[2]\n","        x4 = pvt[3]\n","\n","        # CIM\n","        x1 = self.ca(x1) * x1 # channel attention\n","        cim_feature = self.sa(x1) * x1 # spatial attention\n","\n","\n","        # CFM\n","        x2_t = self.Translayer2_1(x2)\n","        x3_t = self.Translayer3_1(x3)\n","        x4_t = self.Translayer4_1(x4)\n","        cfm_feature = self.CFM(x4_t, x3_t, x2_t)\n","\n","        # SAM\n","        T2 = self.Translayer2_0(cim_feature)\n","        T2 = self.down05(T2)\n","        sam_feature = self.SAM(cfm_feature, T2)\n","\n","        prediction1 = self.out_CFM(cfm_feature)\n","        prediction2 = self.out_SAM(sam_feature)\n","\n","        prediction1_8 = F.interpolate(prediction1, scale_factor=8, mode='bilinear')\n","        prediction2_8 = F.interpolate(prediction2, scale_factor=8, mode='bilinear')\n","        return prediction1_8, prediction2_8\n","\n","class PolypPVTtiny(nn.Module):\n","    def __init__(self, channel=32):\n","        super(PolypPVTtiny, self).__init__()\n","\n","        self.backbone = pvt_v2_b0()  # [64, 128, 320, 512]\n","        path = './pretrained_pth/pvt_v2_b0.pth'\n","        save_model = torch.load(path)\n","        model_dict = self.backbone.state_dict()\n","        state_dict = {k: v for k, v in save_model.items() if k in model_dict.keys()}\n","        model_dict.update(state_dict)\n","        self.backbone.load_state_dict(model_dict)\n","\n","        self.Translayer2_0 = BasicConv2d(32, channel, 1)\n","        self.Translayer2_1 = BasicConv2d(64, channel, 1)\n","        self.Translayer3_1 = BasicConv2d(160, channel, 1)\n","        self.Translayer4_1 = BasicConv2d(256, channel, 1)\n","\n","        self.CFM = CFM(channel)\n","        self.ca = ChannelAttention(32)\n","        self.sa = SpatialAttention()\n","        self.SAM = SAM()\n","\n","        self.down05 = nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=True)\n","        self.out_SAM = nn.Conv2d(channel, 1, 1)\n","        self.out_CFM = nn.Conv2d(channel, 1, 1)\n","\n","\n","    def forward(self, x):\n","\n","        # backbone\n","        pvt = self.backbone(x)\n","        x1 = pvt[0]\n","        x2 = pvt[1]\n","        x3 = pvt[2]\n","        x4 = pvt[3]\n","\n","        # CIM\n","        x1 = self.ca(x1) * x1 # channel attention\n","        cim_feature = self.sa(x1) * x1 # spatial attention\n","\n","\n","        # CFM\n","        x2_t = self.Translayer2_1(x2)\n","        x3_t = self.Translayer3_1(x3)\n","        x4_t = self.Translayer4_1(x4)\n","        cfm_feature = self.CFM(x4_t, x3_t, x2_t)\n","\n","        # SAM\n","        T2 = self.Translayer2_0(cim_feature)\n","        T2 = self.down05(T2)\n","        sam_feature = self.SAM(cfm_feature, T2)\n","\n","        prediction1 = self.out_CFM(cfm_feature)\n","        prediction2 = self.out_SAM(sam_feature)\n","\n","        prediction1_8 = F.interpolate(prediction1, scale_factor=8, mode='bilinear')\n","        prediction2_8 = F.interpolate(prediction2, scale_factor=8, mode='bilinear')\n","        return prediction1_8, prediction2_8\n","\n","\n","if __name__ == '__main__':\n","    model = PolypPVT().cuda()\n","    input_tensor = torch.randn(1, 3, 352, 352).cuda()\n","\n","    prediction1, prediction2 = model(input_tensor)\n","    print(prediction1.size(), prediction2.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"id":"nZceyKd-Pvi0","executionInfo":{"status":"error","timestamp":1727420101190,"user_tz":300,"elapsed":2247,"user":{"displayName":"Tensor Baduwal","userId":"06083011424774211651"}},"outputId":"14f5a429-9ffa-4e51-90d9-f409759b8d3e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-cdefef65a7e5>:164: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  save_model = torch.load(path)\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './pretrained_pth/pvt_v2_b2.pth'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-cdefef65a7e5>\u001b[0m in \u001b[0;36m<cell line: 277>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolypPVT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m352\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m352\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-cdefef65a7e5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, channel)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpvt_v2_b2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [64, 128, 320, 512]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./pretrained_pth/pvt_v2_b2.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0msave_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './pretrained_pth/pvt_v2_b2.pth'"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNi3_MlGOKzH"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import numpy as np\n","import os, argparse\n","from scipy import misc\n","from lib.pvt import PolypPVT, PolypPVTtiny\n","from utils.dataloader import test_dataset\n","import cv2\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--testsize', type=int, default=352, help='testing size')\n","    parser.add_argument('--pth_path', type=str, default='/home/nhdang/PolypKD/model_pth/PolypPVTKD/PolypPVT_KDatt/PolypPVT.pth')\n","    opt = parser.parse_args()\n","    # model = PolypPVTtiny()\n","    model = PolypPVT()\n","    model.load_state_dict(torch.load(opt.pth_path))\n","    model.cuda()\n","    model.eval()\n","    for _data_name in ['CVC-300', 'CVC-ClinicDB', 'Kvasir', 'CVC-ColonDB', 'ETIS-LaribPolypDB']:\n","\n","        ##### put data_path here #####\n","        data_path = './dataset/TestDataset/{}'.format(_data_name)\n","        ##### save_path #####\n","        save_path = './result_map/PolypPVT_vis/{}/'.format(_data_name)\n","\n","        if not os.path.exists(save_path):\n","            os.makedirs(save_path)\n","        image_root = '{}/images/'.format(data_path)\n","        gt_root = '{}/masks/'.format(data_path)\n","        num1 = len(os.listdir(gt_root))\n","        test_loader = test_dataset(image_root, gt_root, 352)\n","\n","        for i in range(num1):\n","            image, gt, name = test_loader.load_data()\n","            gt = np.asarray(gt, np.float32)\n","            gt /= (gt.max() + 1e-8)\n","            image = image.cuda()\n","            P1,P2 = model(image)\n","            adp = torch.nn.AdaptiveAvgPool2d((P1.shape[-1]))\n","\n","            print(\"P1 shape:\",P1.shape, \"P2 shape:\",P2.shape)\n","            # res = F.upsample(P1+P2, size=gt.shape, mode='bilinear', align_corners=False)\n","            res = adp(P1 + torch.transpose(P1, 1,2))\n","            tmp = P1 + torch.transpose(P1, 1,2)\n","            print(\"symmetrical structure:\",tmp.shape)\n","            print(res.shape)\n","            res = res.sigmoid().data.cpu().numpy().squeeze()\n","            # print(res.shape)\n","            out = []\n","            for i in range(10):\n","                res_img = (res[i] - res[i].min()) / (res[i].max() - res[i].min() + 1e-8)\n","                out.append(res_img)\n","            # res = (res - res.min()) / (res.max() - res.min() + 1e-8)\n","\n","            out_img = np.concatenate(out, axis = 1)\n","            cv2.imwrite(save_path+name, out_img*255)\n","        print(_data_name, 'Finish!')"]},{"cell_type":"markdown","source":["# References\n","\n","* Paper: https://arxiv.org/abs/2312.08555\n","* Github: https://github.com/huyquoctrinh/KDAS\n"],"metadata":{"id":"b2QiiBArOPBy"}}]}