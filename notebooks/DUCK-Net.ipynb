{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNE62NfnGqEDOJA7QEYHpIU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Load images"],"metadata":{"id":"uFCqDF-sMemk"}},{"cell_type":"code","source":["import glob\n","\n","import numpy as np\n","from PIL import Image\n","from skimage.io import imread\n","from tqdm import tqdm\n","\n","folder_path = \"\"  # Add the path to your data directory\n","\n","\n","def load_data(img_height, img_width, images_to_be_loaded, dataset):\n","    IMAGES_PATH = folder_path + 'images/'\n","    MASKS_PATH = folder_path + 'masks/'\n","\n","    if dataset == 'kvasir':\n","        train_ids = glob.glob(IMAGES_PATH + \"*.jpg\")\n","\n","    if dataset == 'cvc-clinicdb':\n","        train_ids = glob.glob(IMAGES_PATH + \"*.tif\")\n","\n","    if dataset == 'cvc-colondb' or dataset == 'etis-laribpolypdb':\n","        train_ids = glob.glob(IMAGES_PATH + \"*.png\")\n","\n","    if images_to_be_loaded == -1:\n","        images_to_be_loaded = len(train_ids)\n","\n","    X_train = np.zeros((images_to_be_loaded, img_height, img_width, 3), dtype=np.float32)\n","    Y_train = np.zeros((images_to_be_loaded, img_height, img_width), dtype=np.uint8)\n","\n","    print('Resizing training images and masks: ' + str(images_to_be_loaded))\n","    for n, id_ in tqdm(enumerate(train_ids)):\n","        if n == images_to_be_loaded:\n","            break\n","\n","        image_path = id_\n","        mask_path = image_path.replace(\"images\", \"masks\")\n","\n","        image = imread(image_path)\n","        mask_ = imread(mask_path)\n","\n","        mask = np.zeros((img_height, img_width), dtype=np.bool_)\n","\n","        pillow_image = Image.fromarray(image)\n","\n","        pillow_image = pillow_image.resize((img_height, img_width))\n","        image = np.array(pillow_image)\n","\n","        X_train[n] = image / 255\n","\n","        pillow_mask = Image.fromarray(mask_)\n","        pillow_mask = pillow_mask.resize((img_height, img_width), resample=Image.LANCZOS)\n","        mask_ = np.array(pillow_mask)\n","\n","        for i in range(img_height):\n","            for j in range(img_width):\n","                if mask_[i, j] >= 127:\n","                    mask[i, j] = 1\n","\n","        Y_train[n] = mask\n","\n","    Y_train = np.expand_dims(Y_train, axis=-1)\n","\n","    return X_train, Y_train"],"metadata":{"id":"6qH8ta8WMgQd","executionInfo":{"status":"ok","timestamp":1727419015566,"user_tz":300,"elapsed":1007,"user":{"displayName":"Tensor Baduwal","userId":"06083011424774211651"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Matrice"],"metadata":{"id":"uRsohrx3J9Ji"}},{"cell_type":"code","source":["import keras.backend as K\n","import tensorflow as tf\n","\n","\n","def dice_metric_loss(ground_truth, predictions, smooth=1e-6):\n","    ground_truth = K.cast(ground_truth, tf.float32)\n","    predictions = K.cast(predictions, tf.float32)\n","    ground_truth = K.flatten(ground_truth)\n","    predictions = K.flatten(predictions)\n","    intersection = K.sum(predictions * ground_truth)\n","    union = K.sum(predictions) + K.sum(ground_truth)\n","\n","    dice = (2. * intersection + smooth) / (union + smooth)\n","\n","    return 1 - dice"],"metadata":{"id":"bOx2uLprKAvM","executionInfo":{"status":"ok","timestamp":1727418455381,"user_tz":300,"elapsed":12286,"user":{"displayName":"Tensor Baduwal","userId":"06083011424774211651"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"xIpm_yG1JFPa"}},{"cell_type":"code","source":["# from keras.layers import BatchNormalizationV2, add\n","from tensorflow.keras.layers import BatchNormalization, add # Import BatchNormalization from tensorflow.keras.layers\n","\n","from keras.layers import Conv2D\n","\n","kernel_initializer = 'he_uniform'\n","\n","\n","def conv_block_2D(x, filters, block_type, repeat=1, dilation_rate=1, size=3, padding='same'):\n","    result = x\n","\n","    for i in range(0, repeat):\n","\n","        if block_type == 'separated':\n","            result = separated_conv2D_block(result, filters, size=size, padding=padding)\n","        elif block_type == 'duckv2':\n","            result = duckv2_conv2D_block(result, filters, size=size)\n","        elif block_type == 'midscope':\n","            result = midscope_conv2D_block(result, filters)\n","        elif block_type == 'widescope':\n","            result = widescope_conv2D_block(result, filters)\n","        elif block_type == 'resnet':\n","            result = resnet_conv2D_block(result, filters, dilation_rate)\n","        elif block_type == 'conv':\n","            result = Conv2D(filters, (size, size),\n","                            activation='relu', kernel_initializer=kernel_initializer, padding=padding)(result)\n","        elif block_type == 'double_convolution':\n","            result = double_convolution_with_batch_normalization(result, filters, dilation_rate)\n","\n","        else:\n","            return None\n","\n","    return result\n","\n","\n","def duckv2_conv2D_block(x, filters, size):\n","    x = BatchNormalizationV2(axis=-1)(x)\n","    x1 = widescope_conv2D_block(x, filters)\n","\n","    x2 = midscope_conv2D_block(x, filters)\n","\n","    x3 = conv_block_2D(x, filters, 'resnet', repeat=1)\n","\n","    x4 = conv_block_2D(x, filters, 'resnet', repeat=2)\n","\n","    x5 = conv_block_2D(x, filters, 'resnet', repeat=3)\n","\n","    x6 = separated_conv2D_block(x, filters, size=6, padding='same')\n","\n","    x = add([x1, x2, x3, x4, x5, x6])\n","\n","    x = BatchNormalizationV2(axis=-1)(x)\n","\n","    return x\n","\n","\n","def separated_conv2D_block(x, filters, size=3, padding='same'):\n","    x = Conv2D(filters, (1, size), activation='relu', kernel_initializer=kernel_initializer, padding=padding)(x)\n","\n","    x = BatchNormalizationV2(axis=-1)(x)\n","\n","    x = Conv2D(filters, (size, 1), activation='relu', kernel_initializer=kernel_initializer, padding=padding)(x)\n","\n","    x = BatchNormalizationV2(axis=-1)(x)\n","\n","    return x\n","\n","\n","def midscope_conv2D_block(x, filters):\n","    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n","               dilation_rate=1)(x)\n","\n","    x = BatchNormalizationV2(axis=-1)(x)\n","\n","    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n","               dilation_rate=2)(x)\n","\n","    x = BatchNormalizationV2(axis=-1)(x)\n","\n","    return x\n","\n","\n","def widescope_conv2D_block(x, filters):\n","    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n","               dilation_rate=1)(x)\n","\n","    x = BatchNormalizationV2(axis=-1)(x)\n","\n","    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n","               dilation_rate=2)(x)\n","\n","    x = BatchNormalizationV2(axis=-1)(x)\n","\n","    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n","               dilation_rate=3)(x)\n","\n","    x = BatchNormalizationV2(axis=-1)(x)\n","\n","    return x\n","\n","\n","def resnet_conv2D_block(x, filters, dilation_rate=1):\n","    x1 = Conv2D(filters, (1, 1), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n","                dilation_rate=dilation_rate)(x)\n","\n","    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n","               dilation_rate=dilation_rate)(x)\n","    x = BatchNormalizationV2(axis=-1)(x)\n","    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n","               dilation_rate=dilation_rate)(x)\n","    x = BatchNormalizationV2(axis=-1)(x)\n","    x_final = add([x, x1])\n","\n","    x_final = BatchNormalizationV2(axis=-1)(x_final)\n","\n","    return x_final\n","\n","\n","def double_convolution_with_batch_normalization(x, filters, dilation_rate=1):\n","    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n","               dilation_rate=dilation_rate)(x)\n","    x = BatchNormalizationV2(axis=-1)(x)\n","    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n","               dilation_rate=dilation_rate)(x)\n","    x = BatchNormalizationV2(axis=-1)(x)\n","\n","    return x"],"metadata":{"id":"-rCpQPGELREa","executionInfo":{"status":"ok","timestamp":1727418794872,"user_tz":300,"elapsed":297,"user":{"displayName":"Tensor Baduwal","userId":"06083011424774211651"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from keras.layers import Conv2D, UpSampling2D\n","from keras.layers import add\n","from keras.models import Model\n","\n","# from CustomLayers.ConvBlock2D import conv_block_2D\n","\n","kernel_initializer = 'he_uniform'\n","interpolation = \"nearest\"\n","\n","\n","def create_model(img_height, img_width, input_chanels, out_classes, starting_filters):\n","    input_layer = tf.keras.layers.Input((img_height, img_width, input_chanels))\n","\n","    print('Starting DUCK-Net')\n","\n","    p1 = Conv2D(starting_filters * 2, 2, strides=2, padding='same')(input_layer)\n","    p2 = Conv2D(starting_filters * 4, 2, strides=2, padding='same')(p1)\n","    p3 = Conv2D(starting_filters * 8, 2, strides=2, padding='same')(p2)\n","    p4 = Conv2D(starting_filters * 16, 2, strides=2, padding='same')(p3)\n","    p5 = Conv2D(starting_filters * 32, 2, strides=2, padding='same')(p4)\n","\n","    t0 = conv_block_2D(input_layer, starting_filters, 'duckv2', repeat=1)\n","\n","    l1i = Conv2D(starting_filters * 2, 2, strides=2, padding='same')(t0)\n","    s1 = add([l1i, p1])\n","    t1 = conv_block_2D(s1, starting_filters * 2, 'duckv2', repeat=1)\n","\n","    l2i = Conv2D(starting_filters * 4, 2, strides=2, padding='same')(t1)\n","    s2 = add([l2i, p2])\n","    t2 = conv_block_2D(s2, starting_filters * 4, 'duckv2', repeat=1)\n","\n","    l3i = Conv2D(starting_filters * 8, 2, strides=2, padding='same')(t2)\n","    s3 = add([l3i, p3])\n","    t3 = conv_block_2D(s3, starting_filters * 8, 'duckv2', repeat=1)\n","\n","    l4i = Conv2D(starting_filters * 16, 2, strides=2, padding='same')(t3)\n","    s4 = add([l4i, p4])\n","    t4 = conv_block_2D(s4, starting_filters * 16, 'duckv2', repeat=1)\n","\n","    l5i = Conv2D(starting_filters * 32, 2, strides=2, padding='same')(t4)\n","    s5 = add([l5i, p5])\n","    t51 = conv_block_2D(s5, starting_filters * 32, 'resnet', repeat=2)\n","    t53 = conv_block_2D(t51, starting_filters * 16, 'resnet', repeat=2)\n","\n","    l5o = UpSampling2D((2, 2), interpolation=interpolation)(t53)\n","    c4 = add([l5o, t4])\n","    q4 = conv_block_2D(c4, starting_filters * 8, 'duckv2', repeat=1)\n","\n","    l4o = UpSampling2D((2, 2), interpolation=interpolation)(q4)\n","    c3 = add([l4o, t3])\n","    q3 = conv_block_2D(c3, starting_filters * 4, 'duckv2', repeat=1)\n","\n","    l3o = UpSampling2D((2, 2), interpolation=interpolation)(q3)\n","    c2 = add([l3o, t2])\n","    q6 = conv_block_2D(c2, starting_filters * 2, 'duckv2', repeat=1)\n","\n","    l2o = UpSampling2D((2, 2), interpolation=interpolation)(q6)\n","    c1 = add([l2o, t1])\n","    q1 = conv_block_2D(c1, starting_filters, 'duckv2', repeat=1)\n","\n","    l1o = UpSampling2D((2, 2), interpolation=interpolation)(q1)\n","    c0 = add([l1o, t0])\n","    z1 = conv_block_2D(c0, starting_filters, 'duckv2', repeat=1)\n","\n","    output = Conv2D(out_classes, (1, 1), activation='sigmoid')(z1)\n","\n","    model = Model(inputs=input_layer, outputs=output)\n","\n","    return model"],"metadata":{"id":"4EQgg_qmJx9h","executionInfo":{"status":"ok","timestamp":1727418931551,"user_tz":300,"elapsed":163,"user":{"displayName":"Tensor Baduwal","userId":"06083011424774211651"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"zeR2aVCQIlgC"}},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"T4M5n6RFHJAR","executionInfo":{"status":"error","timestamp":1727419116138,"user_tz":300,"elapsed":192,"user":{"displayName":"Tensor Baduwal","userId":"06083011424774211651"}},"outputId":"b2745d79-0aa1-4aeb-e1dc-624d7f0270d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Num GPUs Available:  0\n","Resizing training images and masks: 0\n"]},{"output_type":"stream","name":"stderr","text":["0it [00:00, ?it/s]\n"]},{"output_type":"error","ename":"ValueError","evalue":"With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-51dbbf711f36>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Splitting the data, seed for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.111\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2784\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2785\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2786\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2787\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2415\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2416\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."]}],"source":["import tensorflow as tf\n","import albumentations as albu\n","import numpy as np\n","import gc\n","import pickle\n","import matplotlib.pyplot as plt\n","from keras.callbacks import CSVLogger\n","from datetime import datetime\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import jaccard_score, precision_score, recall_score, accuracy_score, f1_score\n","# from ModelArchitecture.DiceLoss import dice_metric_loss\n","# from ModelArchitecture import DUCK_Net\n","# from ImageLoader import ImageLoader2D\n","\n","\n","# Checking the number of GPUs available\n","\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","\n","# Setting the model parameters\n","\n","img_size = 352\n","dataset_type = 'kvasir' # Options: kvasir/cvc-clinicdb/cvc-colondb/etis-laribpolypdb\n","learning_rate = 1e-4\n","seed_value = 58800\n","filters = 17 # Number of filters, the paper presents the results with 17 and 34\n","optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n","\n","ct = datetime.now()\n","\n","model_type = \"DuckNet\"\n","\n","progress_path = 'ProgressFull/' + dataset_type + '_progress_csv_' + model_type + '_filters_' + str(filters) +  '_' + str(ct) + '.csv'\n","progressfull_path = 'ProgressFull/' + dataset_type + '_progress_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.txt'\n","plot_path = 'ProgressFull/' + dataset_type + '_progress_plot_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.png'\n","model_path = 'ModelSaveTensorFlow/' + dataset_type + '/' + model_type + '_filters_' + str(filters) + '_' + str(ct)\n","\n","EPOCHS = 600\n","min_loss_for_saving = 0.2\n","\n","\n","# Loading the data\n","\n","X, Y = load_data(img_size, img_size, -1, 'kvasir')\n","\n","# Splitting the data, seed for reproducibility\n","\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, shuffle= True, random_state = seed_value)\n","x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.111, shuffle= True, random_state = seed_value)\n","\n","\n","# Defining the augmentations\n","\n","aug_train = albu.Compose([\n","    albu.HorizontalFlip(),\n","    albu.VerticalFlip(),\n","    albu.ColorJitter(brightness=(0.6,1.6), contrast=0.2, saturation=0.1, hue=0.01, always_apply=True),\n","    albu.Affine(scale=(0.5,1.5), translate_percent=(-0.125,0.125), rotate=(-180,180), shear=(-22.5,22), always_apply=True),\n","])\n","\n","def augment_images():\n","    x_train_out = []\n","    y_train_out = []\n","\n","    for i in range (len(x_train)):\n","        ug = aug_train(image=x_train[i], mask=y_train[i])\n","        x_train_out.append(ug['image'])\n","        y_train_out.append(ug['mask'])\n","\n","    return np.array(x_train_out), np.array(y_train_out)\n","\n","# Creating the model\n","\n","model = DUCK_Net.create_model(img_height=img_size, img_width=img_size, input_chanels=3, out_classes=1, starting_filters=filters)\n","\n","# Compiling the model\n","\n","model.compile(optimizer=optimizer, loss=dice_metric_loss)\n","\n","# Training the model\n","\n","step = 0\n","\n","for epoch in range(0, EPOCHS):\n","\n","    print(f'Training, epoch {epoch}')\n","    print('Learning Rate: ' + str(learning_rate))\n","\n","    step += 1\n","\n","    image_augmented, mask_augmented = augment_images()\n","\n","    csv_logger = CSVLogger(progress_path, append=True, separator=';')\n","\n","    model.fit(x=image_augmented, y=mask_augmented, epochs=1, batch_size=4, validation_data=(x_valid, y_valid), verbose=1, callbacks=[csv_logger])\n","\n","    prediction_valid = model.predict(x_valid, verbose=0)\n","    loss_valid = dice_metric_loss(y_valid, prediction_valid)\n","\n","    loss_valid = loss_valid.numpy()\n","    print(\"Loss Validation: \" + str(loss_valid))\n","\n","    prediction_test = model.predict(x_test, verbose=0)\n","    loss_test = dice_metric_loss(y_test, prediction_test)\n","    loss_test = loss_test.numpy()\n","    print(\"Loss Test: \" + str(loss_test))\n","\n","    with open(progressfull_path, 'a') as f:\n","        f.write('epoch: ' + str(epoch) + '\\nval_loss: ' + str(loss_valid) + '\\ntest_loss: ' + str(loss_test) + '\\n\\n\\n')\n","\n","    if min_loss_for_saving > loss_valid:\n","        min_loss_for_saving = loss_valid\n","        print(\"Saved model with val_loss: \", loss_valid)\n","        model.save(model_path)\n","\n","    del image_augmented\n","    del mask_augmented\n","\n","    gc.collect()\n","\n","\n","# Computing the metrics and saving the results\n","\n","print(\"Loading the model\")\n","\n","model = tf.keras.models.load_model(model_path, custom_objects={'dice_metric_loss':dice_metric_loss})\n","\n","prediction_train = model.predict(x_train, batch_size=4)\n","prediction_valid = model.predict(x_valid, batch_size=4)\n","prediction_test = model.predict(x_test, batch_size=4)\n","\n","print(\"Predictions done\")\n","\n","dice_train = f1_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n","                           np.ndarray.flatten(prediction_train > 0.5))\n","dice_test = f1_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n","                          np.ndarray.flatten(prediction_test > 0.5))\n","dice_valid = f1_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n","                           np.ndarray.flatten(prediction_valid > 0.5))\n","\n","print(\"Dice finished\")\n","\n","\n","miou_train = jaccard_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n","                           np.ndarray.flatten(prediction_train > 0.5))\n","miou_test = jaccard_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n","                          np.ndarray.flatten(prediction_test > 0.5))\n","miou_valid = jaccard_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n","                           np.ndarray.flatten(prediction_valid > 0.5))\n","\n","print(\"Miou finished\")\n","\n","\n","precision_train = precision_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n","                                  np.ndarray.flatten(prediction_train > 0.5))\n","precision_test = precision_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n","                                 np.ndarray.flatten(prediction_test > 0.5))\n","precision_valid = precision_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n","                                  np.ndarray.flatten(prediction_valid > 0.5))\n","\n","print(\"Precision finished\")\n","\n","\n","recall_train = recall_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n","                            np.ndarray.flatten(prediction_train > 0.5))\n","recall_test = recall_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n","                           np.ndarray.flatten(prediction_test > 0.5))\n","recall_valid = recall_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n","                            np.ndarray.flatten(prediction_valid > 0.5))\n","\n","print(\"Recall finished\")\n","\n","\n","accuracy_train = accuracy_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n","                                np.ndarray.flatten(prediction_train > 0.5))\n","accuracy_test = accuracy_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n","                               np.ndarray.flatten(prediction_test > 0.5))\n","accuracy_valid = accuracy_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n","                                np.ndarray.flatten(prediction_valid > 0.5))\n","\n","\n","print(\"Accuracy finished\")\n","\n","\n","final_file = 'results_' + model_type + '_' + str(filters) + '_' + dataset_type + '.txt'\n","print(final_file)\n","\n","with open(final_file, 'a') as f:\n","    f.write(dataset_type + '\\n\\n')\n","    f.write('dice_train: ' + str(dice_train) + ' dice_valid: ' + str(dice_valid) + ' dice_test: ' + str(dice_test) + '\\n\\n')\n","    f.write('miou_train: ' + str(miou_train) + ' miou_valid: ' + str(miou_valid) + ' miou_test: ' + str(miou_test) + '\\n\\n')\n","    f.write('precision_train: ' + str(precision_train) + ' precision_valid: ' + str(precision_valid) + ' precision_test: ' + str(precision_test) + '\\n\\n')\n","    f.write('recall_train: ' + str(recall_train) + ' recall_valid: ' + str(recall_valid) + ' recall_test: ' + str(recall_test) + '\\n\\n')\n","    f.write('accuracy_train: ' + str(accuracy_train) + ' accuracy_valid: ' + str(accuracy_valid) + ' accuracy_test: ' + str(accuracy_test) + '\\n\\n\\n\\n')\n","\n","print('File done')"]},{"cell_type":"markdown","source":["# References\n","\n","* Paper: https://www.nature.com/articles/s41598-023-36940-5\n","* Github: https://github.com/RazvanDu/DUCK-Net"],"metadata":{"id":"iG1lR9TMHllq"}}]}