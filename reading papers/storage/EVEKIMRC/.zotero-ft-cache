UnScene3D: Unsupervised 3D Instance Segmentation for Indoor Scenes
David Rozenberszki1 Or Litany2,3 Angela Dai1
1Technical University of Munich 2Technion 3NVIDIA
https://rozdavid.github.io/unscene3d
Figure 1. We propose UnScene3D, a fully-unsupervised 3D instance segmentation method, effectively separating semantic instances without requiring any manual annotations. We utilize geometric primitives to ensure crisps masks, and due to our self-training loop we can also obtain a dense set of predictions even in cluttered indoor scenarios.
Abstract
3D instance segmentation is fundamental to geometric understanding of the world around us. Existing methods for instance segmentation of 3D scenes rely on supervision from expensive, manual 3D annotations. We propose UnScene3D, the first fully unsupervised 3D learning approach for class-agnostic 3D instance segmentation of indoor scans. UnScene3D first generates pseudo masks by leveraging self-supervised color and geometry features to find potential object regions. We operate on a basis of geometric oversegmentation, enabling efficient representation and learning on high-resolution 3D data. The coarse proposals are then refined through self-training our model on its predictions. Our approach improves over clusteringbased alternatives to unsupervised 3D instance segmentation methods by more than 300% Average Precision score, demonstrating effective instance segmentation even in challenging, cluttered 3D scenes.
1. Introduction
The increasing availability of commodity RGB-D sensors, now widely available on iPhones as well as with the Microsoft Kinect or Intel RealSense, has enabled consumer
level capture of 3D geometry of real-world environments. To enable applications in robotics, autonomous navigation, and mixed reality in such scenes, semantic 3D scene understanding is necessary. In particular, 3D instance segmentation is critical to 3D perception, providing dense instance mask predictions, thus enabling physical and geometric reasoning about objects in an environment. While various 3D deep learning approaches have been developed for 3D instance segmentation [5, 13, 16, 17, 20, 21, 27, 29, 3942, 46–48, 50, 53, 54], they require full supervision from expensive, manual, dense annotations on 3D scenes. We introduce UnScene3D, a novel approach designed for class-agnostic 3D instance segmentation. Our aim is to identify objects in real-world 3D scans by predicting their dense instance masks, without any constraints to a predefined set of class categories. Moreover, we avoid expensive data annotation requirements by operating in an unsupervised fashion, instead leveraging self-supervised 2D and 3D features for segmentation. UnScene3D comprises two essential elements. First, we observe that for RGB-D scan data, self-supervised representation learning methods [18, 56] can provide an innate signal indicating object-ness through feature similarity. We thus generate pseudo masks over 3D segment primitives, based on multimodal analysis of self-supervised color and geometry features from the RGB-D data. By considering
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.
19957


mesh segments rather than voxels or points, our approach efficiently scales with high-resolution 3D data in large scene environments while inherently promoting contiguous segmentation masks. As we require strong features for these initial coarse estimates, we fuse information from both geometric and 2D color features in a complementary fashion. Second, following the pseudo mask generation, we train our model through iterative self-training on both the initial pseudo masks and the current confident model predictions. Through multiple rounds of self-training with noise robust losses achieve improved object recognition and segmentation. At inference time, we do not require any 2D color signal and can produce class-agnostic 3D instance segmentation for a new geometric observation of a 3D environment. Experiments on challenging, cluttered indoor environments from the ScanNet [9], S3DIS [1] and ARKit [2] datasets show that UnScene3D improves significantly over unsupervised, clustering-based state of the art. In summary, our contributions are: • We propose an unsupervised 3D instance segmentation approach for indoor RGB-D scans, without requiring any human annotation. • We generate sparse 3D pseudo masks for unsupervised training based on a multi-modal fusion of color and geometric signal from RGB-D scan data. We achieve robustness and efficiency through a geometry-aware scene coarsening. • Our generated pseudo masks are iteratively refined by self-training for 3D instances to improve 3D instance segmentation performance.
2. Related Work
Self-supervised 3D pretraining While significant progress has been made in fully supervised 3D instance segmentation [8, 13, 15–17, 19, 39, 41, 46, 47] the amount of densely annotated 3D data is scarce. Inspired by success in the 2D domain, various 3D pretraining methods have been developed to boost semantic and instance segmentation performance when fine-tuning with annotated semantic labels. Such methods leverage instance discrimination based on different camera views [18, 56], local augmentations [57], or multiple LIDAR sweeps [36]. While these methods can provide powerful 3D feature extraction, they do not construct any notion of object instances.
Weakly-supervised 3D segmentation Classical methods have leveraged object template information to match or retrieve templates to local geometry in a scene [4, 23, 26, 28, 33, 34], thereby identifying potential object locations. Other methods formulated 3D dense instance segmentation with only 3D box annotation [6, 38] or single-point supervision and active-learning [31, 49]. More recent methods have focused on exploiting knowledge from powerful pre-trained
vision-language models to inform text-guided queries in 3D scenes [11, 22, 30, 37, 43]; however, such methods still rely on large-scale annotated data in the 2D domain.
Clustering-based segmentation There has been very little work done in fully unsupervised 3D instance segmentation, but classical clustering methods have been used to group regions with similar geometric properties together. A particularly notable approach is the densitybased clustering of DBSCAN [12] and its hierarchical counterpart HDBSCAN [32]. These methods can be used to group point clusters in a 3D scene based on point normals and colors. The ScanNet dataset [9] showed that the Felzenswalb algorithm [14] originally developed for image over-segmentation, can generate useful geometric segment clusters. We also exploit such geometric primitives to guide dimensionality reduction and feature aggregation.
Finally, recent methods have been developed to detect instances with self-supervised pretrained features in driving scenarios. These methods often leverage the unique properties of such data including dynamics and instance sparsity. Song et. al. [45] identify object instances through motion, showing promise for self-driving scenarios, but limited to moving objects. Nunes et. al. [35] additionally propose a clustering and graph cut based refinement on pre-trained 3D features, focusing on sparse outdoor scenarios to identify spatially separate objects. Our solution aims to segments instances in complex, cluttered indoor environments.
Unsupervised 2D instance segmentation Classical graph-cut algorithms [7, 10, 44, 55] can be used to detect objects in scenes, employing low-level feature clustering to identify self-similar regions. Recent advances in self-supervised feature learning have been employed in 2D unsupervised instance segmentation methods, which use two-stage training pipelines to achieve remarkable segmentation results [51, 52]. These methods first generate a set of coarse pseudo masks building on the insights of graph-cut algorithms and then refine them with a series of self-training iterations. In particular, FreeSolo [51] uses multi-branch feature extraction to obtain self-similar regions as mask proposals, producing a dense set of initial pseudo-annotated instances. CutLER [52] uses the normalized cut (NCut) algorithm [44] with deep self-supervised features from DINO [3] to identify multiple prominent regions as pseudo masks. Inspired by such approaches we also leverage pseudo mask generation and self-training, but to handle high-dimensional, noisy real-world 3D scan data, we employ a multi-modal feature reasoning and geometric graph coarsening for robust unsupervised 3D instance segmentation.
19958


3. Method
Problem definition We propose an unsupervised learning-based method for 3D instance segmentation. We operate on a set of training 3D scenes {Xi}nt
i=1, represented as mesh graphs G = (V, E), of vertices V and triangular face edges E, where each scene Xi contains an unknown set of ni objects in the ith scene. We aim to train a model that can predict for a previously unseen input scene X, a set of 3D masks representing the different object instances in that scene.
Method overview In order to achieve unsupervised 3D instance segmentation we first break down the scenes into N geometric primitives SN , which we use to initialize an adjacency matrix W to extract an initial set of pseudo masks M 0, representing instance hypotheses based on combining 2D and 3D inputs F2D / F3D ∈ RN×D2D/3D , where D2D, D3D are the dimensions of the 2D/3D selfsupervised features. We regularize the per-segment similarities over geometric primitives for mitigating noise and enabling efficient 3D reasoning. We then employ a series of self-training cycles, updating pseudo mask supervision with new predicted masks, in order to produce final 3D instances. An overview of our approach is shown in Figure 2.
3.1. Initial pseudo mask generation
In order to initiate self-training, we first generate an initial set of pseudo masks, leveraging complementary information from 2D and 3D signal in {Xi}.
3.1.1 Feature aggregation
To encourage effective initial pseudo mask generation, we employ joint reasoning across both self-supervised color and geometry features, as they can provide complementary information regarding objects. As RGB-D scans often contain color image information and reconstructed 3D scan geometry, we can associate both 2D and 3D features in 3D by back-projecting the 2D extracted features using the corresponding depth and camera pose information for each image. Both 2D and 3D features are extracted through stateof-the-art self-supervised feature learning methods [3, 18]. As real-world camera estimation often contains small misalignment errors and noise or oversmoothing in reconstructed scan geometry, these self-supervised features can often also contain high-frequency noise, which we address in Sec. 3.1.2 when reasoning over these features. Note that while we employ both 2D and 3D signal when available for training, we do not require any aligned color image inputs for inference, enabling more general applicability.
3.1.2 3D Graph Cut
To generate pseudo masks from the 2D and 3D selfsupervised features, we employ graph cut to estimate classagnostic instances from the background. More precisely, we leverage the principle of Normalized Cut [44] (NCut), which employs eigenvalue decomposition from an adjacency matrix W ∈ RN×N over a graph to identify selfsimilar regions potentially representing semantic instances, where a set of potential instances can be extracted iteratively. Given a graph representing the 3D scene, we build an adjacency matrix W and self-supervised features with a corresponding degree matrix D ∈ RN×N , where D(i, i) = ΣjW (i, j) and (D − W )v = λDv. In this system, finding the second smallest eigenvalue λ and its corresponding eigenvector v is a close approximation for the minimized cost. From v, we obtain foreground separation by taking all node activations where the eigenvector components were larger than their mean. To identify multiple foreground objects, this process is repeated iteratively. Unfortunately, applying this approach directly to the 3D scenes {Xi} in common 3D representations such as voxels or points is not only computationally infeasible, but unreliable due to the noise in camera pose estimation and geometric reconstruction of 3D scan data. Thus, we propose to regularize the graph cut across geometric primitives.
3.1.3 Geometric Primitives
To employ efficient reasoning across high-dimensional 3D data and enable robust 3D regularization of noisy features, we propose to operate on geometric primitives acquired through a graph coarsening process. For a 3D scene Xi we construct the graph G = (V, E) where V and E being the mesh vertices and face edges. Then, nodes with similar normals and colors are aggregated and clustered based on the mesh topology following [14] and resulting in a set SN = {C1 . . . CN } and S(SN ) = V where Cn represent a single primitive. This reduces the graph size by multiple orders of magnitude, and enables effective regularization of noise in the used self-supervised 2D and 3D features.
3.1.4 NCut on Geometric Primitives
After addressing the challenge of dimensionality reduction and effectively mitigating speckle noise in our features using geometric primitives, we can leverage the capabilities of the Normalized Cut algorithm to achieve a clean partitioning of scene graphs. For this, we iteratively apply NCut to our aggregated features for the extraction of initial pseudo masks denoted as M . Starting with an empty set M 0 = {}, we iteratively compute the adjacency matrix over SN and retrieve the masks m ⊂ SN . We start
19959


Initial Pseudo Masks
Feature Aggregation
+
Self-training Pseudo Masks
Mask & Cut
Input / Output
Final Instances
Self-train Predictions
3D Transformer
3D Backbone
Feature Aggregation
3D Feats.
2D Feats.
Input
Feature Aggregation
+
Mask & Cut
Final Instances
Self-train Predictions
3D Transformer
3D Backbone
Self-supervised Feature Aggregation
3D Feats.
2D Feats.
Input Mesh
Per Segment Feats.
Scene Voxels
Voxel Preds.
Per Voxel Feats.
Per Voxel Feats. Pseudo Masks
Self-Training
Figure 2. UnScene3D first generates a set of pseudo masks (top) to initiate self-training (bottom) for unsupervised 3D instance segmentation. We leverage features from 3D self-supervised pre-training in combination with 2D self-supervised features on an input mesh. These multi-modal features are then aggregated on geometric primitives, integrating low- and high-level signals for pseudo mask segmentation. These initial pseudo masks are then used as supervision for a 3D transformer-based model to produce updated instance masks that are integrated into the supervision of multiple self-training cycles. Finally, we obtain clean and dense instance segmentation without using any manual annotations.
from N geometric segments with their corresponding Ddimensional features F ∈ RN×D, and construct the similarity matrix A = sim(F), where sim denotes cosine similarity. Additionally, for the multi-modal setup we calculate similarity matrices A2D and A3D independently and take their weighted average to obtain the final scores. Empirically, we found this to be more robust than direct feature fusion of the different modalities, due to their different statistical characteristics. We obtain Wj introduced in Section 3.1.2 by thresholding A at τcut, where j denotes the jth NCut iteration. Using Wj, we solve for the second eigenvector vj and threshold it to retrieve the partition mj. We keep all separated foregrounds in M 0, where for each upcoming iteration, we mask out the row and column vectors from Wj, where mi ∈ M 0 was already accepted as a foreground instance and i being the previous segment ids. This allows greedy separation of instances in order of confidence in every cut iteration. Examples of our generated pseudo masks are visualized in Figures 5 and 6. As the adjacency graph is unaware of the mesh connectivity, NCut often results in masks that span spatially separated scene regions. In 3D, we can leverage knowledge of physical distance and connectivity of G to constrain masks to be contiguous in the coarsened scene connectivity graph. We thus filter masks mj that have separated components, keeping only the parts m ̃j that contain the item with the maximum absolute value in vj. Separation based on connectivity is performed before saving m ̃j into M 0, thus allowing for repeated detection of the dropped part over the
next NCut iterations. Finally, we iterate until the maximum number of instances M 0 = {mi}Nm
i=1 are obtained, or there are no segments left in the scene. Moreover, we favor generating a reliable set of masks at the cost of restricting to a sparse initial set (i.e., missing potential instances rather than generating noisy masks for them) through a stricter τcut or lower number of instances.
3.2. Self-Training
Our initial pseudo masks can provide a set of proposed instances M 0; however, these pseudo masks are quite sparse in the scenes and sometimes over- or under-split nearby instances. We thus refine the pseudo mask data through an iterative self-training strategy, producing final instance segmentation predictions M ′ with more dense and complete instance proposals. We leverage a state-of-the-art 3D transformer-based backbone [42] for our self-training from pseudo mask data as mask-head supervision, while the class-head is collapsed to foreground and background classes. Through multiple training cycles we save the proposals of the tth iteration into M t, from the self-trained model, and save these masks as an extension to the original pseudo dataset obtaining M t ⊇ M 0. From the second training iteration, we can extract the most confident K predictions and sample these new instance proposals as an addition to the pseudo annotations. Further, we only accept new instances if the added information value is larger than the minimum threshold, measured by simple segment IoU scores. This way, we can effectively
19960


densify the originally sparse annotations, but without limiting the quality of the originally clean pseudo masks.
3.3. Implementation Details
Backbones. We use a Res16UNet34C sparse-voxel UNet implemented in the MinkowskiEngine [8] for 3D pretrained feature extraction as well as for the 3D transformer during self-training. For the pretrained features we use our own trained weights of [18] for compatibility reasons.
Self-training. We employ the 3D transformer architecture of [42], initialized from scratch. The first self-training cycle is trained for 600 epochs with a batch size of 8 until convergence, which takes ≈ 3 days on a single NVIDIA RTX A6000 GPU. Further self-training cycles are all initialized from the previous state and finetuned for an additional 50 epochs in ≈ 4 hours and for a total of 4 training cycles to produce the final set of instance predictions S. For the Hungarian assignment, we take the original weighted combination of dice and binary cross-entropy losses and only apply the DropLoss condition in the backpropagation phase.
4. Experiments
We demonstrate the effectiveness of UnScene3D for unsupervised class-agnostic 3D instance segmentation on challenging real-world 3D scan datasets containing a large diversity of objects and significant clutter. We train our method and all learned baselines on ScanNet [9], using the official train split. Note that no semantic annotation data is used for training, only the RGB-D reconstructions. Additionally, we show that our approach trained on ScanNet data can effectively transfer to class-agnostic 3D instance segmentation on ARKitScenes [2] data.
Datasets. We train and evaluate UnScene3D on RGB-D scan data from ScanNet [9], using the official train split. We use the raw RGB images, and registered camera poses for training our approach, while the semantic annotations are used only for evaluation. We use the official ScanNet train split for both the pre-trained 3D features from [18] and our self-training iterations. We additionally evaluate our method on ARKitScenes [2], on an 884/120 train/test split of indoor LIDAR scans. For ARKitScenes, we use 3D pre-trained features from ScanNet, followed by pseudo mask generation and self-training on the ARKitScenes train scenes. We convert the LIDAR scan data to meshes with Poisson Surface Reconstruction [24, 25] prior to our graph coarsening. Note that all baselines using learned features are trained on the same ScanNet data as ours.
Evaluation metrics. We evaluate class-agnostic 3D instance segmentation performance with the widely-used Average Precision score on the full-resolution mesh vertices.
ScanNet AP@25 AP@50 AP
HDBSCAN [32] 32.1 5.5 1.6 Nunes et al. [35] 30.5 7.3 2.3 Felzenswalb [14] 38.9 12.7 5.0 CutLER Projection [52] 7.0 0.2 0.3 Ours 58.5 32.2 15.9
Table 1. Unsupervised class-agnostic 3D instance segmentation on ScanNet [9]. Our approach improves significantly over baselines (3x improvement in AP) due to our pseudo mask generation and self-training strategy.
Following the strategy of the supervised benchmark [9] we report scores at IoU scores of 25% and 50% (AP@25, AP@50) and averaged over all overlaps between [50% and 95%] at 5% steps (AP). Note that since predictions are class agnostic, all methods evaluate only instance mask AP values without considering any semantic class labels. For ScanNet, we evaluate against ground truth instance masks from the established 20-class benchmark. Since ARKitScenes does not contain any ground truth instance mask annotations, we evaluate all methods qualitatively.
Comparison to the state of the art. We evaluate our approach in comparison to state-of-the-art traditional clustering methods HDBSCAN [32] and Felzenszwalb’s algorithm [14], in addition to the unsupervised approach of Nunes et. al. [35] leveraging learned feature clustering and refinement. All baselines are provided with input mesh vertices, colors, and normals, while our approach and Nunes et. al. also operate on sparse voxel scene representations. Table 1 and Figure 3 show comparisons on ScanNet data; our UnScene3D approach improves significantly over state of the art by effectively leveraging signal from self-supervised 3D features to guide our model through self-training. Note that since Nunes et. al. has been designed for outdoor applications, even while leveraging ScanNet-trained features, it uses ground removal and relies on physical object separation, making segmentation difficult in cluttered scenes. Additionally, we demonstrate the importance of reasoning in 3D, and compare with a state-of-the-art unsupervised 2D instance segmentation approach CutLER [52] run on the RGB frames of the scans, and projected to 3D using the corresponding camera poses. Here, the difficulty lies in resolving view inconsistencies, occlusions, and lack of knowledge of geometric structure resulting in poor 3D segmentation performance despite plausible 2D proposals.
Evaluation on other datasets We quantitatively evaluate UnScene3D on the Area 5 of the S3DIS dataset [1] using only 3D features pretrained on [9]. Comparison with 3Donly state-of-the-art can be seen in Table 2. We additionally compare with state of the art on ARKitScenes [2] data in Figure 7. Here we show only qualitative
19961


Figure 3. Qualitative comparison on ScanNet [9] scenes with projected predictions from the 2D method CutLER [52], traditional clusteringbased methods Felzenszwalb [14] and HDBSCAN [32], and the GraphCut-based cluster refinement method [35]. Our approach leverages strong pseudo mask prediction and a self-training strategy to produce cleaner, more accurate instance segmentation.
19962


S3DIS AP@25 AP@50 AP
HDBSCAN [32] 27.9 11.2 5.0 Felzenswalb [14] 23.5 10.7 5.0 Nunes et al. [35] 20.1 10.5 5.5 Ours 52.6 40.3 21.4
Table 2. Evaluation on S3DIS dataset (Area 5). UnScene3D is able to adapt to other datasets as well and shows a significant improvement over previous SOTA methods.
results due to the absence of ground truth instance mask annotations. UnScene3D effectively produces cleaner, more accurate segmentations in these complex environments.
UnScene3D as data-efficient pretraining UnScene3D is able to learn powerful object properties and dense segmentation even in a fully unsupervised fashion. We demonstrate the potential of our strong learned features for downstream 3D instance segmentation with limited annotated data. We follow the setup introduced by CSC [18] with limited reconstructions available for downstream fine-tuning. We show our method as a strong pretraining strategy in Figure 4, notably outperforming both training from scratch as well as the state-of-the-art 3D pretraining of CSC. For more details we refer to our supplementary material.
Figure 4. Our unsupervised self-training produces strong 3D features that can served as a powerful pretraining strategy for 3D instance segmentation in limited data scenarios. UnScene3D significantly outperforms state-of-the-art self-supervised 3D pretraining [18] on ScanNet instance segmentation.
What is the effect of multi-modal signal for pseudo mask generation? We evaluate the effect self-supervised color and geometry signals for generating pseudo annotations in Table 3. We consider using only self-supervised geometric features (3D), only self-supervised color features (2D) that are projected to the 3D scans, and both together (both). We find that the color and geometry provide complementary signals. We also note that color features are only used for the initial pseudo mask generation, during self-training iterations and test time only 3D features were used.
Modality AP@25 AP@50 AP AP Final
FreeMask 3D 14.4 3.6 1.3 2.0 Ours 3D 45.4 16.7 9.2 13.3
FreeMask 2D 31.1 15.1 6.8 13.8 Ours 2D 51.3 21.8 9.4 15.7
FreeMask both 23.7 10.1 5.7 12.1 Ours both 52.9 23.2 10.4 15.9
Table 3. We compare pseudo mask generation from 3D-only features (3D), color-only features (2D), and both color and geometry (both) signal, as well as with pseudo annotation generation algorithm FreeMask [51]. In this table we report method performances after a single iteration of self-training initialized from the different pseudo annotation methods and the final AP scores after 4 selftraining iterations.
Figure 5. Initial pseudo masks generated by UnScene3D in comparison with a 3D-lifted FreeMask [51]. FreeMask tends to produce a larger set of noisier pseudo masks, while we rely on a cleaner but sparser set for our self-training.
What is the effect of pseudo annotations? We also evaluate the effect of our pseudo mask generation in Table 3 and Figure 5, in comparison to the 3D adaptation of the FreeMask [51] approach operating on our geometric segments. FreeMask tends to estimate a larger but noisier set of initial pseudo masks, while our approach is focusing on a sparser set of more reliable pseudo masks and produces significantly better performance. The strong difference in performance can be explained by the nature of the samples. While a sparser set of examples can be extended with multiple iterations of self-training, noisy samples will propagate through the full pipeline, and thus directly degrade the final performance. Further details of our adaptations of the FreeMask 3D method can be found in our supplemental.
What is the impact of self-training? We observe that while self-training iterations are always improving the qualitative performance, their effective added information value is saturating after a limited number of cycles. We report on Table 4 through the first 4 steps, and observe a significant relative improvement in both modalities.
19963


Figure 6. UnScene3D employs self-training to refine the initial sparse set of proposals. We can see consistent improvement over both the number of predicted instances and the quality of the instance masks. Here we show results using the pseudo annotations obtained from both modalities.
3D Only 3D & 2D
AP@25 AP@50 AP AP@25 AP@50 AP
S0 pseudo masks 13.8 4.7 2 19.9 10.0 5.9 1st Self-train 45.4 16.7 9.2 52.9 23.2 10.4 2nd Self-train 50.0 24.1 12.0 56.5 29.8 15.0 3rd Self-train 52.2 25.8 12.8 58.8 31.9 15.9 4st Self-train 52.7 26.2 13.3 58.5 32.2 15.9
Table 4. Multiple iterations of self-training significantly improve performance, saturating around 4 iterations.
Limitations While UnScene3D offers a promising step towards unsupervised 3D instance segmentation, various limitations remain. We rely on a mesh representation for graph coarsening, but believe this could be extended to alternative representations through neighborhood reasoning. Additionally, our graph coarsening step may cause very small objects (e.g., pens, cell phones) to be missed in the pseudo annotation generation. Finally, employing a fixed set of pseudo masks from the initial stage that are used
Figure 7. As UnScene3D does not require any human annotation, so we can also train and test our method on the ARKitScenes [2] dataset. We leverages 3D features followed by a series of selftraining iterations for cleaner, more accurate instance segmentation. Qualitative results shows consistently better results than our baselines.
through self-training could reinforce noisy predictions.
5. Conclusion
We introduced UnScene3D, a novel approach towards achieving fully-unsupervised 3D instance segmentation in cluttered indoor scenes. Our approach effectively combined low-level geometric properties to regularize multi-modal self-supervised deep features for initial pseudo mask extraction, and our self-training notably improved performance by refining these proposals to a more complete, dense set of instances. As 3D instance segmentation is a crucial aspect of 3D scene understanding, UnScene3D’s ability to achieve this without requiring any manual annotations opens up new possibilities for 3D semantic understanding.
6. Acknowledgements
This project is funded by the Bavarian State Ministry of Science and the Arts and coordinated by the Bavarian Research Institute for Digital Transformation (bidt), the ERC Starting Grant SpatialSem (101076253), and supported in part by a Google research gift. Or Litany is a Taub fellow and is supported by the Azrieli Foundation Early Career Faculty Fellowship.
19964


References
[1] Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1534–1543, 2016. 2, 5 [2] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. ARKitscenes - a diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. 2, 5, 8
[3] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. 2, 3
[4] Kang Chen, Yu-Kun Lai, Yu-Xin Wu, Ralph Martin, and ShiMin Hu. Automatic semantic modeling of indoor scenes from low-quality rgb-d data using contextual information. ACM Transactions on Graphics, 33(6), 2014. 2
[5] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical aggregation for 3d instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15467–15476, 2021. 1 [6] Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll. Box2mask: Weakly supervised 3d semantic instance segmentation using bounding boxes. In European Conference on Computer Vision (ECCV). Springer, 2022. 2 [7] Sunil Chopra and M. R. Rao. The partition problem. Mathematical Programming, 59:87–115, 1993. 2
[8] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3075–3084, 2019. 2, 5 [9] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. 2, 5, 6 [10] Michel Deza and Monique Laurent. Geometry of cuts and metrics. In Algorithms and Combinatorics, 2009. 2
[11] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Pla: Language-driven openvocabulary 3d scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 2
[12] Martin Ester, Hans-Peter Kriegel, Jo ̈rg Sander, and Xiaowei Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, page 226–231. AAAI Press, 1996. 2
[13] Siqi Fan, Qiulei Dong, Fenghua Zhu, Yisheng Lv, Peijun Ye, and Fei-Yue Wang. Scf-net: Learning spatial contextual features for large-scale point cloud segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14504–14513, 2021. 1, 2 [14] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient graph-based image segmentation. International journal of computer vision, 59:167–181, 2004. 2, 3, 5, 6, 7 [15] Benjamin Graham, Martin Engelcke, and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks. CVPR, 2018. 2 [16] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance segmentation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2937–2946, 2020. 1
[17] Ji Hou, Angela Dai, and Matthias Nießner. 3d-sis: 3d semantic instance segmentation of rgb-d scans. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4421–4430, 2019. 1, 2 [18] Ji Hou, Benjamin Graham, Matthias Nießner, and Saining Xie. Exploring data-efficient 3d scene understanding with contrastive scene contexts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15587–15597, 2021. 1, 2, 3, 5, 7 [19] Ji Hou, Xiaoliang Dai, Zijian He, Angela Dai, and Matthias Nießner. Mask3d: Pre-training 2d vision transformers by learning masked 3d priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13510–13519, 2023. 2 [20] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham. Randla-net: Efficient semantic segmentation of large-scale point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1110811117, 2020. 1 [21] Le Hui, Linghua Tang, Yaqi Shen, Jin Xie, and Jian Yang. Learning superpoint graph cut for 3d instance segmentation. In NeurIPS, 2022. 1 [22] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, Joshua B. Tenenbaum, Celso Miguel de Melo, Madhava Krishna, Liam Paull, Florian Shkurti, and Antonio Torralba. Conceptfusion: Open-set multimodal 3d mapping. arXiv, 2023. 2 [23] Andrej Karpathy, Stephen Miller, and Li Fei-Fei. Object discovery in 3d scenes via shape analysis. In 2013 IEEE international conference on robotics and automation, pages 2088–2095. IEEE, 2013. 2 [24] Michael Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction. ACM Transactions on Graphics (ToG), 32(3):1–13, 2013. 5 [25] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe. Poisson surface reconstruction. In Proceedings of the fourth Eurographics symposium on Geometry processing, page 0, 2006. 5 [26] Young Min Kim, Niloy J Mitra, Dong-Ming Yan, and Leonidas Guibas. Acquiring 3d indoor environments with
19965


variability and repetition. ACM Transactions on Graphics (TOG), 31(6):1–11, 2012. 2 [27] Maksim Kolodiazhnyi, Danila Rukhovich, Anna Vorontsova, and Anton Konushin. Top-down beats bottom-up in 3d instance segmentation, 2023. 1 [28] Yangyan Li, Angela Dai, Leonidas Guibas, and Matthias Nießner. Database-assisted object retrieval for real-time 3d reconstruction. In Computer graphics forum, pages 435446. Wiley Online Library, 2015. 2 [29] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmentation in 3d scenes using semantic superpoint tree networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 27832792, 2021. 1 [30] Minghua Liu, Yinhao Zhu, H. Cai, Shizhong Han, Z. Ling, Fatih Murat Porikli, and Hao Su. Partslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 21736–21746, 2022. 2
[31] Zhengzhe Liu, Xiaojuan Qi, and Chi-Wing Fu. One thing one click: A self-training approach for weakly supervised 3d semantic segmentation. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1726–1736, 2021. 2 [32] Leland McInnes and John Healy. Accelerated hierarchical density based clustering. In 2017 IEEE International Conference on Data Mining Workshops (ICDMW), pages 33–42. IEEE, 2017. 2, 5, 6, 7 [33] Yoshikatsu Nakajima, Byeongkeun Kang, Hideo Saito, and Kris Kitani. Incremental class discovery for semantic segmentation with rgbd sensing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 2
[34] Liangliang Nan, Ke Xie, and Andrei Sharf. A search-classify approach for cluttered indoor scene understanding. ACM Trans. Graph., 31(6), 2012. 2
[35] Lucas Nunes, Xieyuanli Chen, Rodrigo Marcuzzi, Aljosa Osep, Laura Leal-Taixe ́, Cyrill Stachniss, and Jens Behley. Unsupervised class-agnostic instance segmentation of 3d lidar data for autonomous vehicles. IEEE Robotics and Automation Letters, 7(4):8713–8720, 2022. 2, 5, 6, 7 [36] Lucas Nunes, Rodrigo Marcuzzi, Xieyuanli Chen, Jens Behley, and Cyrill Stachniss. Segcontrast: 3d point cloud feature representation learning through self-supervised segment discrimination. IEEE Robotics and Automation Letters, 7(2):2116–2123, 2022. 2 [37] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 815–824, 2023. 2
[38] Yinyin Peng, Hui Feng, Tao Chen, and Bo Hu. Point cloud instance segmentation with inaccurate bounding-box annotations. Sensors (Basel, Switzerland), 23, 2023. 2
[39] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. Advances in neural information processing systems, 30, 2017. 1, 2
[40] Dario Rethage, Johanna Wald, Jurgen Sturm, Nassir Navab, and Federico Tombari. Fully-convolutional point networks for large-scale point clouds. In Proceedings of the European Conference on Computer Vision (ECCV), pages 596611, 2018. [41] David Rozenberszki, Or Litany, and Angela Dai. Languagegrounded indoor 3d semantic segmentation in the wild. In Proceedings of the European Conference on Computer Vision (ECCV), 2022. 2
[42] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3D for 3D Semantic Instance Segmentation. In International Conference on Robotics and Automation (ICRA), 2023. 1, 4, 5
[43] Nur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel Pinto, Soumith Chintala, and Arthur Szlam. Clip-fields: Weakly supervised semantic fields for robotic memory. arXiv preprint arXiv: Arxiv-2210.05663, 2022. 2
[44] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on pattern analysis and machine intelligence, 22(8):888–905, 2000. 2, 3 [45] Ziyang Song and Bo Yang. OGC: Unsupervised 3D Object Segmentation from Rigid Dynamics of Point Clouds. In NeurIPS, 2022. 2
[46] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer for 3d scene instance segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2393–2401, 2023. 1, 2 [47] Thang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, Junyeong Kim, and Chang D Yoo. Softgroup++: Scalable 3d instance segmentation with octree pyramid grouping. arXiv preprint arXiv:2209.08263, 2022. 2
[48] Thang Vu, Kookhoi Kim, Tung M. Luu, Xuan Thanh Nguyen, and Chang D. Yoo. Softgroup for 3d instance segmentation on 3d point clouds. In CVPR, 2022. 1 [49] Puzuo Wang, Wei Yao, and Jie Shao. One class one click: Quasi scene-level weakly supervised point cloud semantic segmentation with active learning. ISPRS Journal of Photogrammetry and Remote Sensing, 204:89–104, 2023. 2
[50] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity group proposal network for 3d point cloud instance segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2569–2578, 2018. 1 [51] Xinlong Wang, Zhiding Yu, Shalini De Mello, Jan Kautz, Anima Anandkumar, Chunhua Shen, and Jose M Alvarez. Freesolo: Learning to segment objects without annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14176–14186, 2022. 2, 7 [52] Xudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31243134, 2023. 2, 5, 6 [53] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic
19966


graph cnn for learning on point clouds. Acm Transactions On Graphics (tog), 38(5):1–12, 2019. 1 [54] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 9621–9630, 2019. 1 [55] Z. Wu and R. Leahy. An optimal graph theoretic approach to data clustering: theory and its application to image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 15(11):1101–1113, 1993. 2 [56] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pretraining for 3d point cloud understanding. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16, pages 574–591. Springer, 2020. 1, 2 [57] Zaiwei Zhang, Rohit Girdhar, Armand Joulin, and Ishan Misra. Self-supervised pretraining of 3d features on any point-cloud. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10252–10263, 2021. 2
19967